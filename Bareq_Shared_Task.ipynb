{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boO3laZyxoAI"
   },
   "source": [
    "# **Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:04.743915Z",
     "start_time": "2025-07-14T02:16:00.923492Z"
    },
    "id": "JSkgLFXk-ZZh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.28.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from neo4j) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rageh\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rageh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install required packages\n",
    "!pip install neo4j datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:07.158159Z",
     "start_time": "2025-07-14T02:16:05.022554Z"
    },
    "id": "A3eqXgoQjCPN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rageh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from neo4j import GraphDatabase\n",
    "from itertools import combinations\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:07.185820Z",
     "start_time": "2025-07-14T02:16:07.174740Z"
    },
    "id": "j7_8l5E9-0pF"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# URI = userdata.get(\"NEO4J_URI\")\n",
    "# USERNAME = userdata.get(\"NEO4J_USERNAME\")\n",
    "# PASSWORD = userdata.get(\"NEO4J_PASSWORD\")\n",
    "URI = os.getenv(\"NEO4J_URI\")\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "neo4j_driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:07.225660Z",
     "start_time": "2025-07-14T02:16:07.219577Z"
    },
    "collapsed": true,
    "id": "vgPh1gVcAlK0"
   },
   "outputs": [],
   "source": [
    "def execute_query(query, parameters=None):\n",
    "\twith neo4j_driver.session() as session:\n",
    "\t\tresult = session.run(query, parameters or {})\n",
    "\t\treturn [record for record in result]\n",
    "# test_query = \"MATCH (n) RETURN n\"\n",
    "# execute_query(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mNdWQ3bvtQD"
   },
   "source": [
    "# **Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "2qwVbSjfeh5N"
   },
   "outputs": [],
   "source": [
    "SAMER_df = pd.read_csv('data/raw/SAMER-Readability-Lexicon-v1.tsv', sep='\\t')\n",
    "\n",
    "df_dev= pd.read_csv('data/raw/dev.csv')\n",
    "\n",
    "data_set = pd.read_csv(\"hf://datasets/CAMeL-Lab/BAREC-Shared-Task-2025-sent/\" + \"train.csv\")\n",
    "\n",
    "data_set = pd.DataFrame(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRbqrmXsx2Af"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JrB5rvPXrdTh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Readability_Level</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Text_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10100290001</td>\n",
       "      <td>مجلة كل الأولاد وكل البنات</td>\n",
       "      <td>7-zay</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10100290002</td>\n",
       "      <td>ماجد</td>\n",
       "      <td>1-alif</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10100290003</td>\n",
       "      <td>الأربعاء 21 يناير 1987</td>\n",
       "      <td>8-Ha</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10100290004</td>\n",
       "      <td>الموافق 21 جمادى الأول 1407هــ</td>\n",
       "      <td>7-zay</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10100290005</td>\n",
       "      <td>السنة الثامنة</td>\n",
       "      <td>5-ha</td>\n",
       "      <td>Arts &amp; Humanities</td>\n",
       "      <td>Foundational</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                        Sentence Readability_Level  \\\n",
       "0  10100290001      مجلة كل الأولاد وكل البنات             7-zay   \n",
       "1  10100290002                            ماجد            1-alif   \n",
       "2  10100290003          الأربعاء 21 يناير 1987              8-Ha   \n",
       "3  10100290004  الموافق 21 جمادى الأول 1407هــ             7-zay   \n",
       "4  10100290005                   السنة الثامنة              5-ha   \n",
       "\n",
       "              Domain    Text_Class  \n",
       "0  Arts & Humanities  Foundational  \n",
       "1  Arts & Humanities  Foundational  \n",
       "2  Arts & Humanities  Foundational  \n",
       "3  Arts & Humanities  Foundational  \n",
       "4  Arts & Humanities  Foundational  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Unwanted Columns\n",
    "data_set_cleaned = data_set.drop(columns=[\"Word_Count\", \"Readability_Level_19\", \"Readability_Level_7\", \"Readability_Level_5\", \"Readability_Level_3\", \"Annotator\", \"Document\", \"Source\", \"Book\", \"Author\"])\n",
    "\n",
    "# Remove Dublicate Rows\n",
    "data_set_cleaned = data_set_cleaned.drop_duplicates(subset='Sentence', keep='first')\n",
    "\n",
    "# Save the cleaned dataset to a CSV file\n",
    "data_set_cleaned.to_csv(\"cleaned_data_set.csv\", index=False) \n",
    "\n",
    "data_set_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rcWRhGcPwfqq"
   },
   "outputs": [],
   "source": [
    "# Function to remove diacritics from Arabic text\n",
    "def remove_diacritics(text):\n",
    "\tarabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652\\u0670]')\n",
    "\treturn re.sub(arabic_diacritics, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oQNymz_ItE4I"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Gloss</th>\n",
       "      <th>readability (rounded average)</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335409</td>\n",
       "      <td>in</td>\n",
       "      <td>1</td>\n",
       "      <td>في</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>270096</td>\n",
       "      <td>from</td>\n",
       "      <td>1</td>\n",
       "      <td>من</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181283</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "      <td>أن</td>\n",
       "      <td>conj_sub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178560</td>\n",
       "      <td>on;above#on_+_what/which</td>\n",
       "      <td>1</td>\n",
       "      <td>على</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157818</td>\n",
       "      <td>to;towards</td>\n",
       "      <td>1</td>\n",
       "      <td>إلى</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Occurrences                     Gloss  readability (rounded average) lemma  \\\n",
       "0       335409                        in                              1    في   \n",
       "1       270096                      from                              1    من   \n",
       "2       181283                      that                              2    أن   \n",
       "3       178560  on;above#on_+_what/which                              1   على   \n",
       "4       157818                to;towards                              1   إلى   \n",
       "\n",
       "        pos  \n",
       "0      prep  \n",
       "1      prep  \n",
       "2  conj_sub  \n",
       "3      prep  \n",
       "4      prep  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unwanted columns\n",
    "SAMER_df = SAMER_df.drop(columns=['Hindawi (5594310)', 'Giga (5594256)','Answer1 - Egyptian', 'Answer2 - Syrian','Answer3 - Saudi Arabian'])\n",
    "\n",
    "# Split 'lemma#pos' into separate columns\n",
    "SAMER_df[['lemma', 'pos']] = SAMER_df['lemma#pos'].str.split('#', expand=True)\n",
    "\n",
    "# Remove the original 'lemma#pos' column\n",
    "SAMER_df = SAMER_df.drop(columns=['lemma#pos'])\n",
    "\n",
    "# Remove diacritics from the 'lemma' column\n",
    "SAMER_df['lemma'] = SAMER_df['lemma'].apply(remove_diacritics)\n",
    "\n",
    "# Remove duplicates based on the 'lemma' column\n",
    "SAMER_df = SAMER_df.drop_duplicates(subset='lemma', keep='first')\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "SAMER_df.to_csv(\"cleaned_SAMER_df.csv\", index=False)\n",
    "\n",
    "SAMER_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY0NcnIoybNn"
   },
   "source": [
    "# Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmcX9wG5y4l5"
   },
   "outputs": [],
   "source": [
    "# Generating Lemmas\n",
    "\n",
    "for i in range(len(SAMER_df)):\n",
    "\tlemma = SAMER_df.iloc[i][\"lemma\"]\n",
    "\tpos = SAMER_df.iloc[i][\"pos\"]\n",
    "\tavg_readability = SAMER_df.iloc[i][\"readability (rounded average)\"]\n",
    "\tfreq = SAMER_df.iloc[i][\"Occurrences\"]\n",
    "\n",
    "\tlemma_query = \"\"\"MERGE (l:Lemma {lemma: $lemma}) ON CREATE SET l.pos = $pos, l.avg_readability = $avg_readability, l.freq = $freq\"\"\"\n",
    "\n",
    "\tlemma_params = {\"lemma\": lemma, \"pos\": pos, \"avg_readability\": avg_readability, \"freq\": freq}\n",
    "\n",
    "\texecute_query(lemma_query, lemma_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Sentences\n",
    "\n",
    "lemma_set = set(SAMER_df['lemma'].astype(str))\n",
    "\n",
    "# to store pairs of lemmas\n",
    "pairs_list = [] \n",
    "for i in range(len(data_set_cleaned)):\n",
    "\tsentence = data_set_cleaned.iloc[i][\"Sentence\"]\n",
    "\tdomain_type = data_set_cleaned.iloc[i][\"Domain\"]\n",
    "\tclass_type = data_set_cleaned.iloc[i][\"Text_Class\"]\n",
    "\n",
    "\twords = re.findall(r'\\b[\\w]+\\b', sentence)\n",
    "\tsentence_to_lemma = []\n",
    "\tfor word in words:\n",
    "\t\tif word in lemma_set:\n",
    "\t\t\tsentence_to_lemma.append(word)\n",
    "\t\t\t\n",
    "\tpairs = [list(pair) for pair in combinations(set(sentence_to_lemma), 2)]\n",
    "\tpairs_list.extend(pairs)\n",
    "\t\n",
    "\tsentence_query = \"\"\"\n",
    "\t\tMERGE (S:Sentence {id: $id})\n",
    "\t\tON CREATE SET S.text = $text\n",
    "\n",
    "\t\tWITH S\n",
    "\t\tMERGE (D:Domain {type: $domainType})\n",
    "\t\tMERGE (C:Class {type: $classType})\n",
    "\t\tMERGE (S)-[:IN_DOMAIN]->(D)\n",
    "\t\tMERGE (S)-[:IN_CLASS]->(C)\n",
    "\n",
    "\t\tWITH S\n",
    "\t\tUNWIND $lemmas AS lemma\n",
    "\t\tMATCH (L:Lemma {lemma: lemma})\n",
    "\t\tMERGE (S)-[r:HAS_LEMMA]->(L)\n",
    "\t\tON CREATE SET r.count = 1\n",
    "\t\tON MATCH SET r.count = r.count + 1\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tsentence_params = {\"id\": i + 1, \"text\": sentence, \"domainType\": domain_type, \"classType\": class_type, \"lemmas\": sentence_to_lemma}\n",
    "\t\n",
    "\texecute_query(sentence_query, sentence_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Pairs of Lemmas\n",
    "lemmas_pairs_query = \"\"\"\n",
    "\t\tUNWIND $pairs AS pair\n",
    "\t\tMATCH (l1:Lemma {lemma: pair[0]})\n",
    "\t\tMATCH (l2:Lemma {lemma: pair[1]})\n",
    "\n",
    "\t\tMERGE (l1)-[r1:OCCUR_WITH]->(l2)\n",
    "\t\tON CREATE SET r1.count = 1\n",
    "\t\tON MATCH SET r1.count = r1.count + 1\n",
    "\n",
    "\t\tMERGE (l2)-[r2:OCCUR_WITH]->(l1)\n",
    "\t\tON CREATE SET r2.count = 1\n",
    "\t\tON MATCH SET r2.count = r2.count + 1\"\"\"\n",
    "\n",
    "lemmas_pairs_params = {\"pairs\": pairs_list}\n",
    "execute_query(lemmas_pairs_query, lemmas_pairs_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_query = \"\"\"MATCH (s:Sentence) RETURN s AS Sentence\"\"\"\n",
    "sentence_records = execute_query(sentence_query)\n",
    "\n",
    "sentence_nodes = []\n",
    "for record in sentence_records:\n",
    "\tlemma_node = record['Sentence']\n",
    "\tsentence = {\n",
    "\t\t\"sentence_text\": record['Sentence'][\"text\"],\n",
    "\t}\n",
    "\tsentence_nodes.append(sentence)\n",
    "\t\t\n",
    "with open(\"data/json/sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(sentence_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_query = \"\"\"MATCH (l:Lemma) RETURN l AS Lemma\"\"\"\n",
    "lemma_records = execute_query(lemmas_query)\n",
    "\n",
    "print(lemma_records)\n",
    "\n",
    "lemma_nodes = []\n",
    "for record in lemma_records:\n",
    "\tlemma_node = record['Lemma']\n",
    "\tlemma = {\n",
    "\t\t\"lemma\": lemma_node[\"lemma\"],\n",
    "\t\t\"pos\": lemma_node[\"pos\"],\n",
    "\t\t\"avg_readability\": lemma_node[\"avg_readability\"],\n",
    "\t\t\"freq\": lemma_node[\"freq\"]\n",
    "\t}\n",
    "\tlemma_nodes.append(lemma)\n",
    "\n",
    "with open(\"data/json/lemmas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(lemma_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lemma_query = \"\"\"MATCH (s:Sentence)-[r:HAS_LEMMA]->(l:Lemma) RETURN s AS sentence, r AS relation, l AS lemma\"\"\"\n",
    "sentence_lemma_records = execute_query(sentence_lemma_query)\n",
    "\n",
    "sentence_lemma_nodes = []\n",
    "for record in sentence_lemma_records:\n",
    "\tsentence_part = record[\"sentence\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tlemma_part = record[\"lemma\"]\n",
    "\tsentence_lemma = {\n",
    "\t\t\"sentence_text\": sentence_part[\"text\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"lemma\": lemma_part[\"lemma\"]\n",
    "\t}\n",
    "\tsentence_lemma_nodes.append(sentence_lemma)\n",
    "\n",
    "with open(\"data/json/sentence_lemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(sentence_lemma_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_lemma_query = \"\"\"MATCH (l1:Lemma)-[r:OCCUR_WITH]->(l2:Lemma)\n",
    "WHERE l1.lemma < l2.lemma \n",
    "RETURN l1 AS lemma1, r AS relation, l2 AS lemma2\"\"\"\n",
    "lemma_lemma_records = execute_query(lemma_lemma_query)\n",
    "\n",
    "\n",
    "lemma_lemma_nodes = []\n",
    "for record in lemma_lemma_records:\n",
    "\tlemma1_part = record[\"lemma1\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tlemma2_part = record[\"lemma2\"]\n",
    "\tlemma_lemma = {\n",
    "\t\t\"lemma1\": lemma1_part[\"lemma\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"count\": relation_part[\"count\"],\n",
    "\t\t\"lemma2\": lemma2_part[\"lemma\"]\n",
    "\t}\n",
    "\tlemma_lemma_nodes.append(lemma_lemma)\n",
    "\n",
    "with open(\"data/json/lemma_lemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(lemma_lemma_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_class_query = \"\"\"MATCH (s:Sentence)-[r:IN_CLASS]->(c:Class) RETURN s AS sentence, r AS relation, c AS class\"\"\"\n",
    "sentence_class_records = execute_query(sentence_class_query)\n",
    "\n",
    "sentence_class_nodes = []\n",
    "for record in sentence_class_records:\n",
    "\tsentence_part = record[\"sentence\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tclass_part = record[\"class\"]\n",
    "\tsentence_class = {\n",
    "\t\t\"sentence_text\": sentence_part[\"text\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"class_type\": class_part[\"type\"]\n",
    "\t}\n",
    "\tsentence_class_nodes.append(sentence_class)\n",
    "with open(\"data/json/sentence_class.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(sentence_class_nodes, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oRbqrmXsx2Af"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
