{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boO3laZyxoAI"
   },
   "source": [
    "# **Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:04.743915Z",
     "start_time": "2025-07-14T02:16:00.923492Z"
    },
    "id": "JSkgLFXk-ZZh"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install neo4j datasets dotenv pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:24:19.948139Z",
     "start_time": "2025-07-25T13:24:14.221713Z"
    },
    "id": "A3eqXgoQjCPN"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from neo4j import GraphDatabase\n",
    "from itertools import combinations\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "db = MorphologyDB.builtin_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:07.185820Z",
     "start_time": "2025-07-14T02:16:07.174740Z"
    },
    "id": "j7_8l5E9-0pF"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# URI = userdata.get(\"NEO4J_URI\")\n",
    "# USERNAME = userdata.get(\"NEO4J_USERNAME\")\n",
    "# PASSWORD = userdata.get(\"NEO4J_PASSWORD\")\n",
    "URI = os.getenv(\"NEO4J_URI\")\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "neo4j_driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:16:07.225660Z",
     "start_time": "2025-07-14T02:16:07.219577Z"
    },
    "collapsed": true,
    "id": "vgPh1gVcAlK0"
   },
   "outputs": [],
   "source": [
    "def execute_query(query, parameters=None):\n",
    "\twith neo4j_driver.session() as session:\n",
    "\t\tresult = session.run(query, parameters or {})\n",
    "\t\treturn [record for record in result]\n",
    "test_query = \"MATCH (n) RETURN n\"\n",
    "execute_query(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mNdWQ3bvtQD"
   },
   "source": [
    "# **Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2qwVbSjfeh5N"
   },
   "outputs": [],
   "source": [
    "# SAMER_df = pd.read_csv('data/raw/SAMER-Readability-Lexicon-v1.tsv', sep='\\t')\n",
    "\n",
    "# df_dev= pd.read_csv('data/raw/dev.csv')\n",
    "\n",
    "# data_set = pd.read_csv(\"hf://datasets/CAMeL-Lab/BAREC-Shared-Task-2025-sent/\" + \"train.csv\")\n",
    "\n",
    "# data_set = pd.DataFrame(data_set)\n",
    "\n",
    "test_sent = pd.read_csv(\"data/raw/test_sent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRbqrmXsx2Af"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrB5rvPXrdTh"
   },
   "outputs": [],
   "source": [
    "# # Remove Unwanted Columns\n",
    "# data_set_cleaned = data_set.drop(columns=[\"Word_Count\", \"Readability_Level_19\", \"Readability_Level_7\", \"Readability_Level_5\", \"Readability_Level_3\", \"Annotator\", \"Document\", \"Source\", \"Book\", \"Author\"])\n",
    "\n",
    "# # Remove Dublicate Rows\n",
    "# data_set_cleaned = data_set_cleaned.drop_duplicates(subset='Sentence', keep='first')\n",
    "\n",
    "# # Save the cleaned dataset to a CSV file\n",
    "# data_set_cleaned.to_csv(\"cleaned_data_set.csv\", index=False) \n",
    "\n",
    "# data_set_cleaned.head()\n",
    "test_sent = test_sent.drop(columns=[\"Word_Count\", \"Annotator\", \"Document\", \"Source\", \"Book\", \"Author\", \"ID\"])\n",
    "\n",
    "test_sent = test_sent[test_sent[\"Sentence\"] != \"#NAME?\"]\n",
    "# test_sent.to_csv(\"cleaned_test_set.csv\", index=False) \n",
    "\n",
    "test_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:15:32.503965Z",
     "start_time": "2025-07-25T14:15:32.491920Z"
    },
    "id": "rcWRhGcPwfqq"
   },
   "outputs": [],
   "source": [
    "# Function to remove diacritics from Arabic text\n",
    "def remove_diacritics(text):\n",
    "\tarabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652\\u0670]')\n",
    "\treturn re.sub(arabic_diacritics, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQNymz_ItE4I"
   },
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "SAMER_df = SAMER_df.drop(columns=['Hindawi (5594310)', 'Giga (5594256)','Answer1 - Egyptian', 'Answer2 - Syrian','Answer3 - Saudi Arabian'])\n",
    "\n",
    "# Split 'lemma#pos' into separate columns\n",
    "SAMER_df[['lemma', 'pos']] = SAMER_df['lemma#pos'].str.split('#', expand=True)\n",
    "\n",
    "# Remove the original 'lemma#pos' column\n",
    "SAMER_df = SAMER_df.drop(columns=['lemma#pos'])\n",
    "\n",
    "# Remove diacritics from the 'lemma' column\n",
    "SAMER_df['lemma'] = SAMER_df['lemma'].apply(remove_diacritics)\n",
    "\n",
    "# Remove duplicates base|d on the 'lemma' column\n",
    "SAMER_df = SAMER_df.drop_duplicates(subset='lemma', keep='first')\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "SAMER_df.to_csv(\"cleaned_SAMER_df.csv\", index=False)\n",
    "\n",
    "SAMER_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY0NcnIoybNn"
   },
   "source": [
    "# Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmcX9wG5y4l5"
   },
   "outputs": [],
   "source": [
    "# Generating Lemmas\n",
    "\n",
    "for i in range(len(SAMER_df)):\n",
    "\tlemma = SAMER_df.iloc[i][\"lemma\"]\n",
    "\tpos = SAMER_df.iloc[i][\"pos\"]\n",
    "\tavg_readability = SAMER_df.iloc[i][\"readability (rounded average)\"]\n",
    "\tfreq = SAMER_df.iloc[i][\"Occurrences\"]\n",
    "\n",
    "\tlemma_query = \"\"\"MERGE (l:Lemma {lemma: $lemma}) ON CREATE SET l.pos = $pos, l.avg_readability = $avg_readability, l.freq = $freq\"\"\"\n",
    "\n",
    "\tlemma_params = {\"lemma\": lemma, \"pos\": pos, \"avg_readability\": avg_readability, \"freq\": freq}\n",
    "\n",
    "\texecute_query(lemma_query, lemma_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Sentences\n",
    "\n",
    "# lemma_set = set(SAMER_df['lemma'].astype(str))\n",
    "\n",
    "# to store pairs of lemmas\n",
    "# pairs_list = [] \n",
    "for i in range(len(test_sent)):\n",
    "\tsentence = test_sent.iloc[i][\"Sentence\"]\n",
    "\tdomain_type = test_sent.iloc[i][\"Domain\"]\n",
    "\tclass_type = test_sent.iloc[i][\"Text_Class\"]\n",
    "\n",
    "\n",
    "\tsentence = remove_diacritics(sentence)\n",
    "\n",
    "\tsentence_to_lemma = re.findall(r'\\b[\\w]+\\b', sentence)\n",
    "\n",
    "\tsentence_to_lemma = [word for word in sentence_to_lemma if not word.isdigit() and len(word) > 1]\n",
    "\n",
    "\tt_sentence = \" \".join(sentence_to_lemma)\n",
    "\n",
    "\t# for word in words:\n",
    "\t# \tif word in lemma_set:\n",
    "\t# \t\tsentence_to_lemma.append(word)\n",
    "\t\t\t\n",
    "\t# pairs = [list(pair) for pair in combinations(set(sentence_to_lemma), 2)]\n",
    "\t# pairs_list.extend(pairs)\n",
    "\t\n",
    "\tsentence_query = \"\"\"\n",
    "\t\tMERGE (S:Sentence {id: $id})\n",
    "\t\tON CREATE SET S.text = $text\n",
    "\n",
    "\t\tWITH S\n",
    "\t\tMERGE (D:Domain {type: $domainType})\n",
    "\t\tMERGE (C:Class {type: $classType})\n",
    "\t\tMERGE (S)-[:IN_DOMAIN]->(D)\n",
    "\t\tMERGE (S)-[:IN_CLASS]->(C)\n",
    "\n",
    "\t\tWITH S\n",
    "\t\tUNWIND $lemmas AS lemma\n",
    "\t\tMATCH (L:Lemma {lemma: lemma})\n",
    "\t\tMERGE (S)-[r:HAS_LEMMA]->(L)\n",
    "\t\tON CREATE SET r.count = 1\n",
    "\t\tON MATCH SET r.count = r.count + 1\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tsentence_params = {\"id\": i + 1, \"text\": t_sentence, \"domainType\": domain_type, \"classType\": class_type, \"lemmas\": sentence_to_lemma}\n",
    "\t\n",
    "\texecute_query(sentence_query, sentence_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generating Pairs of Lemmas\n",
    "# lemmas_pairs_query = \"\"\"\n",
    "# \t\tUNWIND $pairs AS pair\n",
    "# \t\tMATCH (l1:Lemma {lemma: pair[0]})\n",
    "# \t\tMATCH (l2:Lemma {lemma: pair[1]})\n",
    "\n",
    "# \t\tMERGE (l1)-[r1:OCCUR_WITH]->(l2)\n",
    "# \t\tON CREATE SET r1.count = 1\n",
    "# \t\tON MATCH SET r1.count = r1.count + 1\n",
    "\n",
    "# \t\tMERGE (l2)-[r2:OCCUR_WITH]->(l1)\n",
    "# \t\tON CREATE SET r2.count = 1\n",
    "# \t\tON MATCH SET r2.count = r2.count + 1\"\"\"\n",
    "\n",
    "# lemmas_pairs_params = {\"pairs\": pairs_list}\n",
    "# execute_query(lemmas_pairs_query, lemmas_pairs_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_query = \"\"\"MATCH (s:Sentence) RETURN s AS Sentence\"\"\"\n",
    "sentence_records = execute_query(sentence_query)\n",
    "\n",
    "sentence_nodes = []\n",
    "for record in sentence_records:\n",
    "\tlemma_node = record['Sentence']\n",
    "\tsentence = {\n",
    "\t\t\"sentence_text\": record['Sentence'][\"text\"],\n",
    "\t}\n",
    "\tsentence_nodes.append(sentence)\n",
    "\t\t\n",
    "with open(\"data/json/sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(sentence_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_query = \"\"\"MATCH (l:Lemma) RETURN l AS Lemma\"\"\"\n",
    "lemma_records = execute_query(lemmas_query)\n",
    "\n",
    "print(lemma_records)\n",
    "\n",
    "lemma_nodes = []\n",
    "for record in lemma_records:\n",
    "\tlemma_node = record['Lemma']\n",
    "\tlemma = {\n",
    "\t\t\"lemma\": lemma_node[\"lemma\"],\n",
    "\t\t\"pos\": lemma_node[\"pos\"],\n",
    "\t\t\"avg_readability\": lemma_node[\"avg_readability\"],\n",
    "\t\t\"freq\": lemma_node[\"freq\"]\n",
    "\t}\n",
    "\tlemma_nodes.append(lemma)\n",
    "\n",
    "with open(\"data/json/lemmas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(lemma_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_lemma_query = \"\"\"MATCH (s:Sentence)-[r:HAS_LEMMA]->(l:Lemma) RETURN s AS sentence, r AS relation, l AS lemma\"\"\"\n",
    "sentence_lemma_records = execute_query(sentence_lemma_query)\n",
    "\n",
    "sentence_lemma_nodes = []\n",
    "for record in sentence_lemma_records:\n",
    "\tsentence_part = record[\"sentence\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tlemma_part = record[\"lemma\"][\"lemma\"]\n",
    "\n",
    "\tif analyzer.analyze(lemma_part):\n",
    "\t\tanalyses = analyzer.analyze(lemma_part)\n",
    "\t\tlemma = analyses[0]['lex']\n",
    "\telse:\n",
    "\t\tlemma = lemma_part\n",
    "\n",
    "\tlemma = remove_diacritics(lemma)\n",
    "\n",
    "\tsentence_lemma = {\n",
    "\t\t\"sentence_text\": sentence_part[\"text\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"lemma\": lemma\n",
    "\t}\n",
    "\tsentence_lemma_nodes.append(sentence_lemma)\n",
    "\n",
    "with open(\"sentence_lemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(sentence_lemma_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_lemma_query = \"\"\"MATCH (l1:Lemma)-[r:OCCUR_WITH]->(l2:Lemma)\n",
    "WHERE l1.lemma < l2.lemma \n",
    "RETURN l1 AS lemma1, r AS relation, l2 AS lemma2\"\"\"\n",
    "lemma_lemma_records = execute_query(lemma_lemma_query)\n",
    "\n",
    "\n",
    "lemma_lemma_nodes = []\n",
    "for record in lemma_lemma_records:\n",
    "\tlemma1_part = record[\"lemma1\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tlemma2_part = record[\"lemma2\"]\n",
    "\tlemma_lemma = {\n",
    "\t\t\"lemma1\": lemma1_part[\"lemma\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"count\": relation_part[\"count\"],\n",
    "\t\t\"lemma2\": lemma2_part[\"lemma\"]\n",
    "\t}\n",
    "\tlemma_lemma_nodes.append(lemma_lemma)\n",
    "\n",
    "with open(\"data/json/lemma_lemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(lemma_lemma_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_class_records = execute_query(sentence_class_query)\n",
    "\n",
    "sentence_class_nodes = []\n",
    "for record in sentence_class_records:\n",
    "\tsentence_part = record[\"sentence\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tclass_part = record[\"class\"]\n",
    "\tsentence_class = {\n",
    "\t\t\"sentence_text\": sentence_part[\"text\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"class_type\": class_part[\"type\"]\n",
    "\t}\n",
    "\tsentence_class_nodes.append(sentence_class)\n",
    "\t\n",
    "with open(\"sentence_class.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(sentence_class_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_domain_query = \"\"\"MATCH (s:Sentence)-[r:IN_DOMAIN]->(d:Domain) RETURN s AS sentence, r AS relation, d AS domain\"\"\"\n",
    "sentence_domain_records = execute_query(sentence_domain_query)\n",
    "\n",
    "sentence_domain_nodes = []\n",
    "for record in sentence_domain_records:\n",
    "\tsentence_part = record[\"sentence\"]\n",
    "\trelation_part = record[\"relation\"]\n",
    "\tdomain_part = record[\"domain\"]\n",
    "\tsentence_domain = {\n",
    "\t\t\"sentence_text\": sentence_part[\"text\"],\n",
    "\t\t\"relation\": relation_part.type,\n",
    "\t\t\"domain_type\": domain_part[\"type\"]\n",
    "\t}\n",
    "\tsentence_domain_nodes.append(sentence_domain)\n",
    "\t\n",
    "with open(\"sentence_domain.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tjson.dump(sentence_domain_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "sentence_domain_nodes = []\n",
    "\n",
    "# Read CSV file\n",
    "with open(\"data/raw/test_sent.csv\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sentence_text = row[\"Sentence\"]\n",
    "        domain_type = row[\"Domain\"]\n",
    "\n",
    "        sentence_domain = {\n",
    "            \"sentence_text\": sentence_text,\n",
    "            \"relation\": \"IN_DOMAIN\",\n",
    "            \"domain_type\": domain_type\n",
    "        }\n",
    "\n",
    "        sentence_domain_nodes.append(sentence_domain)\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"sentence_domain.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_domain_nodes, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(db)\n",
    "\n",
    "# Test analysis\n",
    "analyses = analyzer.analyze('الطلاب')\n",
    "lemma = analyses[0]['lex']\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:27:39.577591Z",
     "start_time": "2025-07-25T14:27:37.628546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over the JSON data\n",
    "with open(\"data/json/test_doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\ttest_doc_data = json.load(f)\n",
    "\n",
    "updated_items = []\n",
    "for item in test_doc_data:\n",
    "    id = item.get(\"ID\")\n",
    "\n",
    "    sentence = item.get(\"Sentence\")\n",
    "\n",
    "    matching_row = test_doc[test_doc['ID'] == id]\n",
    "\n",
    "    domain = matching_row[\"Domain\"].iloc[0]\n",
    "\n",
    "    sentence = remove_diacritics(sentence)\n",
    "\n",
    "    sentence = re.findall(r'\\b[\\w]+\\b', sentence)\n",
    "\n",
    "    sentence = [word for word in sentence if not word.isdigit() and len(word) > 1]\n",
    "\n",
    "    t_sentence = \" \".join(sentence)\n",
    "\n",
    "    item['domain_type'] = domain\n",
    "    item['Sentence'] = t_sentence\n",
    "    item[\"relation\"] = \"IN_DOMAIN\"\n",
    "\n",
    "    updated_items.append(item)\n",
    "\n",
    "# Save the updated data to a new JSON file\n",
    "with open(\"sentence_domain_doc.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(updated_items, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:27:40.752529Z",
     "start_time": "2025-07-25T14:27:39.611546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over the JSON data\n",
    "\n",
    "with open(\"data/json/test_doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\ttest_doc_data = json.load(f)\n",
    "\n",
    "updated_items = []\n",
    "for item in test_doc_data:\n",
    "    id = item.get(\"ID\")\n",
    "\n",
    "    sentence = item.get(\"Sentence\")\n",
    "\n",
    "    matching_row = test_doc[test_doc['ID'] == id]\n",
    "\n",
    "    text_class = matching_row[\"Text_Class\"].iloc[0]\n",
    "\n",
    "    sentence = remove_diacritics(sentence)\n",
    "\n",
    "    sentence = re.findall(r'\\b[\\w]+\\b', sentence)\n",
    "\n",
    "    sentence = [word for word in sentence if not word.isdigit() and len(word) > 1]\n",
    "\n",
    "    t_sentence = \" \".join(sentence)\n",
    "\n",
    "    item['class_type'] = text_class\n",
    "    item['Sentence'] = t_sentence\n",
    "    item[\"relation\"] = \"IN_CLASS\"\n",
    "\n",
    "    updated_items.append(item)\n",
    "\n",
    "# Save the updated data to a new JSON file\n",
    "with open(\"sentence_class_doc.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(updated_items, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:47:24.525611Z",
     "start_time": "2025-07-25T14:47:18.024177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over the JSON data\n",
    "\n",
    "with open(\"data/json/sentence_lemma.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\tsentence_lemma_data = json.load(f)\n",
    "\n",
    "\n",
    "with open(\"sentence_class_doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\tsentnece_class_doc_data = json.load(f)\n",
    "\n",
    "updated_items = []\n",
    "for item in sentence_lemma_data:\n",
    "    sentence = item.get(\"sentence_text\") # sentence in sentnece_lemma.json\n",
    "\n",
    "    # matching_row = test_doc[test_doc['ID'] == id]\n",
    "\t#\n",
    "    # text_class = matching_row[\"Text_Class\"].iloc[0]\n",
    "\t#\n",
    "    # sentence = remove_diacritics(sentence)\n",
    "\t#\n",
    "    # sentence = re.findall(r'\\b[\\w]+\\b', sentence)\n",
    "\t#\n",
    "    # sentence = [word for word in sentence if not word.isdigit() and len(word) > 1]\n",
    "\n",
    "    # t_sentence = \" \".join(sentence)\n",
    "\n",
    "    result = next((item for item in sentnece_class_doc_data if item.get(\"Sentence\") == sentence), None)\n",
    "\n",
    "    ID = result[\"ID\"]\n",
    "\n",
    "    lemma = remove_diacritics(item[\"lemma\"])\n",
    "\n",
    "    x = {\n",
    "        \"relation\": \"HAS_LEMMA\",\n",
    "        \"ID\": ID,\n",
    "\t\t\"Sentence\": sentence,\n",
    "        \"lemma\": lemma,\n",
    "\t}\n",
    "    updated_items.append(x)\n",
    "\n",
    "# Save the updated data to a new JSON file\n",
    "with open(\"sentence_lemma_doc.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(updated_items, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oRbqrmXsx2Af"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "camel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
